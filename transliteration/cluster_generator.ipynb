{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We proposed 2 methods to generate clusters, using just orthograph or using also embeddings and semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from Levenshtein import distance\n",
    "from typing import Dict, Set\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_PATH = '../data/processed_chats'\n",
    "ONLY_TEXT_DATA_PATH = '../data/only_text_chats'\n",
    "CLUSTER_OUTPUT_PATH = 'normal_clusters.json'\n",
    "SEMANTIC_CLUSTER_OUTPUT_PATH = 'semantic_clusters.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_chats():\n",
    "\t\t# Create output diarectory if it doesn't exist\n",
    "\t\tos.makedirs(ONLY_TEXT_DATA_PATH, exist_ok=True)\n",
    "\t\t\n",
    "\t\t# Get all chat files\n",
    "\t\tchat_files = [f for f in os.listdir('../data/processed_chats') if f.endswith('.txt')]\n",
    "\t\t\n",
    "\t\tfor chat_file in chat_files:\n",
    "\t\t\t\tinput_path = os.path.join(ORIGINAL_DATA_PATH, chat_file)\n",
    "\t\t\t\toutput_path = os.path.join(ONLY_TEXT_DATA_PATH, chat_file)\n",
    "\t\t\t\t\n",
    "\t\t\t\twith open(input_path, 'r', encoding='utf-8') as f:\n",
    "\t\t\t\t\t\tlines = f.readlines()\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t# Extract only the text part after the timestamp and speaker\n",
    "\t\t\t\ttext_lines = []\n",
    "\t\t\t\tfor line in lines:\n",
    "\t\t\t\t\t\t# Match pattern: [timestamp] speaker: text\n",
    "\t\t\t\t\t\tmatch = re.match(r'^\\[.*?\\]\\s+.*?:\\s+(.+)$', line)\n",
    "\t\t\t\t\t\tif match:\n",
    "\t\t\t\t\t\t\t\ttext = match.group(1).strip()\n",
    "\t\t\t\t\t\t\t\tif text:  # Only add non-empty messages\n",
    "\t\t\t\t\t\t\t\t\t\ttext_lines.append(text + '\\n')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Write only the text to new file\n",
    "\t\t\t\twith open(output_path, 'w', encoding='utf-8') as f:\n",
    "\t\t\t\t\t\tf.writelines(text_lines)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tprint(f\"Processed {chat_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chat_1.txt\n",
      "Processed chat_2.txt\n",
      "Processed chat_3.txt\n",
      "Processed chat_6.txt\n",
      "Processed chat_4.txt\n"
     ]
    }
   ],
   "source": [
    "extract_text_from_chats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_similar_words(threshold: float = 0.8) -> Dict[str, Set[str]]:\n",
    "    # Get all words from files\n",
    "    words = set()\n",
    "    chat_dir = '../data/only_text_chats'\n",
    "    \n",
    "    for filename in os.listdir(chat_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(chat_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                text = f.read().lower()\n",
    "                # Extract words, keeping only alphanumeric characters\n",
    "                file_words = re.findall(r'\\b\\w+\\b', text)\n",
    "                words.update(file_words)\n",
    "    \n",
    "    # Convert to list for easier indexing\n",
    "    words = list(words)\n",
    "    clusters = defaultdict(set)\n",
    "    \n",
    "    # Compare each word with every other word\n",
    "    for i, word1 in enumerate(words):\n",
    "        if word1 in clusters:  # Skip if already clustered\n",
    "            continue\n",
    "            \n",
    "        clusters[word1].add(word1)  # Add word to its own cluster\n",
    "        \n",
    "        for word2 in words[i+1:]:\n",
    "            if word2 in clusters:  # Skip if already clustered\n",
    "                continue\n",
    "                \n",
    "            # Calculate similarity ratio\n",
    "            max_len = max(len(word1), len(word2))\n",
    "            if max_len == 0:\n",
    "                continue\n",
    "                \n",
    "            similarity = 1 - (distance(word1, word2) / max_len)\n",
    "            \n",
    "            if similarity >= threshold:\n",
    "                clusters[word1].add(word2)\n",
    "                clusters[word2].add(word1)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = cluster_similar_words(threshold=0.8)\n",
    "\n",
    "with open(CLUSTER_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump({k: sorted(list(v)) for k, v in clusters.items() if len(v) >= 2}, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Cluster Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joehachem/Desktop/ML_FINAL/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/joehachem/Desktop/ML_FINAL/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from Levenshtein import distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Dict, Set\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_semantic_words(spelling_threshold: float = 0.8, semantic_threshold: float = 0.7):\n",
    "    \"\"\"\n",
    "    Cluster words based on both spelling and semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        spelling_threshold: Minimum spelling similarity ratio (0-1)\n",
    "        semantic_threshold: Minimum semantic similarity ratio (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping each word to its cluster of similar words\n",
    "    \"\"\"\n",
    "    # Get all words from files\n",
    "    print(\"Reading files and extracting words...\")\n",
    "    words = set()\n",
    "    chat_dir = '../data/only_text_chats'\n",
    "    \n",
    "    for filename in os.listdir(chat_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(chat_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                text = f.read().lower()\n",
    "                file_words = re.findall(r'\\b\\w+\\b', text)\n",
    "                words.update(file_words)\n",
    "    \n",
    "    words = list(words)\n",
    "    print(f\"Found {len(words)} unique words\")\n",
    "    \n",
    "    # Load multilingual model\n",
    "    print(\"Loading multilingual model...\")\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    \n",
    "    # Get semantic embeddings\n",
    "    print(\"Getting semantic embeddings...\")\n",
    "    embeddings = model.encode(words, show_progress_bar=True)\n",
    "    \n",
    "    # Calculate semantic similarity matrix\n",
    "    print(\"Calculating semantic similarities...\")\n",
    "    semantic_sim = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Initialize clusters\n",
    "    clusters = defaultdict(set)\n",
    "    \n",
    "    # Compare each word with every other word\n",
    "    print(\"Clustering words...\")\n",
    "    total_pairs = len(words) * (len(words) - 1) // 2\n",
    "    processed_pairs = 0\n",
    "    \n",
    "    with tqdm(total=total_pairs, desc=\"Processing word pairs\") as pbar:\n",
    "        for i, word1 in enumerate(words):\n",
    "            if word1 in clusters:  # Skip if already clustered\n",
    "                pbar.update(len(words) - i - 1)\n",
    "                continue\n",
    "                \n",
    "            clusters[word1].add(word1)  # Add word to its own cluster\n",
    "            \n",
    "            for j, word2 in enumerate(words[i+1:], i+1):\n",
    "                if word2 in clusters:  # Skip if already clustered\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate spelling similarity\n",
    "                max_len = max(len(word1), len(word2))\n",
    "                if max_len == 0:\n",
    "                    continue\n",
    "                    \n",
    "                spelling_sim = 1 - (distance(word1, word2) / max_len)\n",
    "                \n",
    "                # Get semantic similarity\n",
    "                semantic_sim_ratio = semantic_sim[i, j]\n",
    "                \n",
    "                # Combine both similarities (you can adjust weights)\n",
    "                combined_sim = (spelling_sim + semantic_sim_ratio) / 2\n",
    "                \n",
    "                if combined_sim >= semantic_threshold:\n",
    "                    clusters[word1].add(word2)\n",
    "                    clusters[word2].add(word1)\n",
    "                \n",
    "                processed_pairs += 1\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clusters_to_json(clusters: Dict[str, Set[str]], min_cluster_size: int = 2):\n",
    "    \"\"\"Save clusters with at least min_cluster_size words to JSON\"\"\"\n",
    "    # Convert sets to lists for JSON serialization\n",
    "    filtered_clusters = {\n",
    "        word: sorted(list(cluster))\n",
    "        for word, cluster in clusters.items()\n",
    "        if len(cluster) >= min_cluster_size\n",
    "    }\n",
    "    \n",
    "    output_file = 'semantic_clusters.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_clusters, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(filtered_clusters)} clusters to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = cluster_semantic_words(spelling_threshold=0.8, semantic_threshold=0.7)\n",
    "save_clusters_to_json(clusters, min_cluster_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
