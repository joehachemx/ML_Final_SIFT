{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joehachem/Desktop/ML_FINAL/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/joehachem/Desktop/ML_FINAL/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_DATA_PATH = '../data/clusters/normal_clusters.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLUSTER_DATA_PATH, 'r') as f:\n",
    "    clusters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "all_words = []\n",
    "cluster_ids = list(clusters.keys())\n",
    "\n",
    "for cluster_id in cluster_ids:\n",
    "    words = clusters[cluster_id]\n",
    "    all_words.extend(words)\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words)):\n",
    "            anchor, positive = words[i], words[j]\n",
    "            # Sample a negative word from a different cluster\n",
    "            negative_cluster = random.choice([c for c in cluster_ids if c != cluster_id])\n",
    "            negative = random.choice(clusters[negative_cluster])\n",
    "            examples.append(InputExample(texts=[anchor, positive, negative]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12741' max='12741' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12741/12741 53:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.870100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.634900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.327700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.138200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Load and fine-tune a multilingual embedding model\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "train_dataloader = DataLoader(examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.TripletLoss(model=model)\n",
    "model.fit([(train_dataloader, train_loss)], epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('clusters_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(clusters, model):\n",
    "    centroids = {}\n",
    "    for cluster_id, words in clusters.items():\n",
    "        embeddings = model.encode(words)\n",
    "        centroid = np.mean(embeddings, axis=0)\n",
    "        centroids[cluster_id] = centroid\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_word(word, centroids, model, threshold=0.6):\n",
    "    word_embedding = model.encode([word])[0]\n",
    "    similarities = {\n",
    "        cid: cosine_similarity([word_embedding], [centroid])[0][0]\n",
    "        for cid, centroid in centroids.items()\n",
    "    }\n",
    "    best_match = max(similarities, key=similarities.get)\n",
    "    best_score = similarities[best_match]\n",
    "\n",
    "    print(\"best_match\", best_match, \"best_score\", best_score)\n",
    "    \n",
    "    if best_score >= threshold:\n",
    "        return best_match\n",
    "    else:\n",
    "        return \"new_cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLUSTER_DATA_PATH, 'r') as f:\n",
    "    clusters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "model = SentenceTransformer('../models/clusters_model')\n",
    "centroids = compute_centroids(clusters, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_match joee best_score 0.9437474\n",
      "Word 'joe' assigned to cluster: joee\n"
     ]
    }
   ],
   "source": [
    "# Test word\n",
    "word = \"joe\"\n",
    "assigned_cluster = classify_word(word, centroids, model)\n",
    "print(f\"Word '{word}' assigned to cluster: {assigned_cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_new_cluster(word: str, cluster_id: str, clusters: dict, model: SentenceTransformer, centroids: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Add a new word to an existing cluster or create a new cluster\n",
    "    \n",
    "    Args:\n",
    "        word: The word to add\n",
    "        cluster_id: The cluster ID to add to (or 'new_cluster' to create new)\n",
    "        clusters: Current clusters dictionary\n",
    "        model: The embedding model\n",
    "        centroids: Current centroids dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Updated clusters dictionary\n",
    "    \"\"\"\n",
    "    if cluster_id == \"new_cluster\":\n",
    "        # Create new cluster ID\n",
    "        new_id = str(max([int(k) for k in clusters.keys()]) + 1)\n",
    "        clusters[new_id] = [word]\n",
    "        # Update centroid\n",
    "        centroids[new_id] = model.encode([word])[0]\n",
    "    else:\n",
    "        # Check if word already exists in cluster\n",
    "        if word in clusters[cluster_id]:\n",
    "            print(f\"Word '{word}' already exists in cluster {cluster_id}\")\n",
    "            return clusters\n",
    "            \n",
    "        # Add to existing cluster\n",
    "        clusters[cluster_id].append(word)\n",
    "        # Update centroid\n",
    "        embeddings = model.encode(clusters[cluster_id])\n",
    "        centroids[cluster_id] = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    # Save updated clusters\n",
    "    with open('clusters.json', 'w') as f:\n",
    "        json.dump(clusters, f, indent=2)\n",
    "    \n",
    "    return clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
